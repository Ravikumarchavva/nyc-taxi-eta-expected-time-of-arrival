{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import polars as pl\n",
    "# import torch\n",
    "# import torch.nn as nn\n",
    "# import torch.optim as optim\n",
    "# from torch.utils.data import Dataset, DataLoader, random_split\n",
    "# from sklearn.preprocessing import StandardScaler\n",
    "# from sklearn.metrics import mean_squared_error, r2_score\n",
    "# import os\n",
    "# import numpy as np\n",
    "# from tqdm import tqdm\n",
    "\n",
    "# # Check for GPU\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# print(f\"Using device: {device}\")\n",
    "\n",
    "\n",
    "# # 1. Define the Dataset Class\n",
    "# class PolarsDataset(Dataset):\n",
    "#     def __init__(self, X_files, y_files, scaler=None):\n",
    "#         self.X_files = X_files\n",
    "#         self.y_files = y_files\n",
    "#         self.scaler = scaler or StandardScaler()\n",
    "#         self.fitted = False  # Track if the scaler is fitted\n",
    "\n",
    "#     def __len__(self):\n",
    "#         return len(self.X_files)\n",
    "\n",
    "#     def __getitem__(self, idx):\n",
    "#         # Load a single batch\n",
    "#         X_batch = pl.read_parquet(self.X_files[idx]).to_pandas()\n",
    "#         y_batch = pl.read_parquet(self.y_files[idx]).to_pandas().squeeze()\n",
    "        \n",
    "#         # Standardize features\n",
    "#         if not self.fitted:\n",
    "#             X_batch = self.scaler.fit_transform(X_batch)\n",
    "#             self.fitted = True\n",
    "#         else:\n",
    "#             X_batch = self.scaler.transform(X_batch)\n",
    "        \n",
    "#         X_tensor = torch.tensor(X_batch, dtype=torch.float32)\n",
    "#         y_tensor = torch.tensor(y_batch.values, dtype=torch.float32).unsqueeze(1)\n",
    "        \n",
    "#         return X_tensor, y_tensor\n",
    "\n",
    "\n",
    "# # 2. Define the PyTorch Model\n",
    "# class SimpleMLP(nn.Module):\n",
    "#     def __init__(self, input_size):\n",
    "#         super(SimpleMLP, self).__init__()\n",
    "#         self.hidden1 = nn.Linear(input_size, 10)\n",
    "#         self.hidden2 = nn.Linear(10, 5)\n",
    "#         self.output = nn.Linear(5, 1)\n",
    "#         self.relu = nn.ReLU()\n",
    "#         self.bn1 = nn.BatchNorm1d(10)\n",
    "#         self.dropout = nn.Dropout(0.2)\n",
    "#         self.bn2 = nn.BatchNorm1d(5)\n",
    "#         self.bn3 = nn.BatchNorm1d(1)\n",
    "        \n",
    "#     def forward(self, x):\n",
    "#         x = self.relu(self.hidden1(x))\n",
    "#         x = self.bn1(x)\n",
    "#         x = self.dropout(x)\n",
    "#         x = self.relu(self.hidden2(x))\n",
    "#         x = self.bn2(x)\n",
    "#         x = self.dropout(x)\n",
    "#         return self.output(x)\n",
    "\n",
    "\n",
    "# # 3. List all batch files\n",
    "# X_files = sorted([f\"../processed_data/features/{f}\" for f in os.listdir('../processed_data/features') if f.endswith('.parquet')])\n",
    "# y_files = sorted([f\"../processed_data/lables/{f}\" for f in os.listdir('../processed_data/lables') if f.endswith('.parquet')])\n",
    "# assert len(X_files) == len(y_files), \"Mismatch between X and y batch files\"\n",
    "\n",
    "\n",
    "# # 4. Initialize Dataset and Split into Train/Test\n",
    "# scaler = StandardScaler()\n",
    "# dataset = PolarsDataset(X_files, y_files, scaler)\n",
    "\n",
    "# train_size = int(0.8 * len(dataset))\n",
    "# test_size = len(dataset) - train_size\n",
    "# train_dataset, test_dataset = random_split(dataset, [train_size, test_size])\n",
    "\n",
    "# train_dataloader = DataLoader(train_dataset, batch_size=1, shuffle=True)\n",
    "# test_dataloader = DataLoader(test_dataset, batch_size=1, shuffle=False)\n",
    "\n",
    "\n",
    "# # 5. Initialize Model, Loss, and Optimizer\n",
    "# input_size = pl.read_parquet(X_files[0]).shape[1]\n",
    "# model = SimpleMLP(input_size).to(device)\n",
    "# criterion = nn.MSELoss()\n",
    "# optimizer = optim.Adam(model.parameters(), lr=0.3)\n",
    "\n",
    "\n",
    "# # 6. Training Loop with Multiple Epochs\n",
    "# EPOCHS = 20  # Number of epochs\n",
    "\n",
    "# for epoch in range(EPOCHS):\n",
    "#     model.train()\n",
    "#     epoch_loss = 0.0\n",
    "#     all_preds = []\n",
    "#     all_targets = []\n",
    "    \n",
    "#     for batch_idx, (X_batch, y_batch) in enumerate(tqdm(train_dataloader, desc=f\"Epoch {epoch+1}/{EPOCHS}\", unit=\"batch\")):\n",
    "#         # Squeeze out the extra dimension\n",
    "#         X_batch = X_batch.squeeze(0)\n",
    "#         y_batch = y_batch.squeeze(0)\n",
    "#         X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "        \n",
    "#         # Forward Pass\n",
    "#         optimizer.zero_grad()\n",
    "#         outputs = model(X_batch)\n",
    "#         loss = criterion(outputs, y_batch)\n",
    "            \n",
    "#         # Backward Pass\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "        \n",
    "#         epoch_loss += loss.item()\n",
    "        \n",
    "#         # Store predictions and targets for RÂ² score\n",
    "#         preds = outputs.detach().cpu().numpy().reshape(-1, 1)\n",
    "#         targs = y_batch.detach().cpu().numpy().reshape(-1, 1)\n",
    "#         all_preds.append(preds)\n",
    "#         all_targets.append(targs)\n",
    "    \n",
    "#     # Calculate average loss and RÂ² score\n",
    "#     avg_loss = epoch_loss / len(train_dataloader)\n",
    "#     all_preds = np.vstack(all_preds)\n",
    "#     all_targets = np.vstack(all_targets)\n",
    "#     r2 = r2_score(all_targets, all_preds)\n",
    "    \n",
    "#     print(f\"Epoch {epoch+1}/{EPOCHS} - Average Loss: {avg_loss:.4f} - RÂ² Score: {r2:.4f}\")\n",
    "\n",
    "\n",
    "# # 7. Evaluation on Test Set\n",
    "# model.eval()\n",
    "# all_preds = []\n",
    "# all_targets = []\n",
    "\n",
    "# with torch.no_grad():\n",
    "#     for X_batch, y_batch in tqdm(test_dataloader, desc=\"Evaluating on Test Set\", unit=\"batch\"):\n",
    "#         X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "#         outputs = model(X_batch)\n",
    "        \n",
    "#         preds = outputs.cpu().numpy().reshape(-1, 1)\n",
    "#         targs = y_batch.cpu().numpy().reshape(-1, 1)\n",
    "#         all_preds.append(preds)\n",
    "#         all_targets.append(targs)\n",
    "\n",
    "# # Aggregate Predictions and Targets\n",
    "# all_preds = np.vstack(all_preds)\n",
    "# all_targets = np.vstack(all_targets)\n",
    "\n",
    "# # Calculate Final Test Metrics\n",
    "# mse = mean_squared_error(all_targets, all_preds)\n",
    "# r2 = r2_score(all_targets, all_preds)\n",
    "\n",
    "# print(\"\\nTraining Complete!\")\n",
    "# print(f\"ðŸ“Š Final Test MSE: {mse:.4f}\")\n",
    "# print(f\"ðŸ“ˆ Final Test RÂ² Score: {r2:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "eda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
